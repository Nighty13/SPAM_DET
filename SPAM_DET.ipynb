{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SPAM_DET.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nighty13/SPAM_DET/blob/master/SPAM_DET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "0T_FXgfQDQR0",
        "colab_type": "code",
        "outputId": "beb556cc-4228-47e6-e324-7dbd5317a9de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import nltk\n",
        "import sklearn\n",
        "import pandas\n",
        "import numpy\n",
        "\n",
        "print('Python : {}'.format(sys.version))\n",
        "print('NLTK: {}'.format(nltk.__version__))\n",
        "print('Scikit-learn: {}'.format(sklearn.__version__))\n",
        "print('Pandas : {}'.format(pandas.__version__))\n",
        "print('Numpy : {}'.format(numpy.__version__))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python : 3.6.7 (default, Oct 22 2018, 11:32:17) \n",
            "[GCC 8.2.0]\n",
            "NLTK: 3.2.5\n",
            "Scikit-learn: 0.19.2\n",
            "Pandas : 0.22.0\n",
            "Numpy : 1.14.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g0tA49yLGAhA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kl1hPou4KYP3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pprint import pprint\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mzWDL0ewKY3A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZaGKdGDnKaTg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _create_file_request(file_id):\n",
        "    return drive_service.files().get_media(fileId=file_id)\n",
        "\n",
        "\n",
        "def _download_response_bytes(request, print_progress=False):\n",
        "    downloaded = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(downloaded, request)\n",
        "    \n",
        "    for status in _progbar(downloader):\n",
        "        if print_progress:\n",
        "            print(\"Downloaded {}/{} bytes\".format(status.resumable_progress, status.total_size))\n",
        "  \n",
        "    downloaded.seek(0)\n",
        "    return downloaded.read()\n",
        "\n",
        "  \n",
        "def _progbar(downloader):\n",
        "    done = False\n",
        "    while done is False:\n",
        "        status, done = downloader.next_chunk()\n",
        "        yield status\n",
        "\n",
        "\n",
        "def get_file_id(name):\n",
        "    return get_matching_files(name)[0]['id']\n",
        "\n",
        "\n",
        "def move_from_drive_to_disk(file_names, file_destinations):\n",
        "    for file_name, dest in zip(file_names, file_destinations):\n",
        "        file_id = get_file_id(file_name)\n",
        "        print('Downloading file: \"{}\"'.format(file_name))\n",
        "        file_bytes = _download_response_bytes(_create_file_request(file_id), print_progress=True)\n",
        "        with open(dest, \"wb\") as f:\n",
        "            f.write(file_bytes)\n",
        "\n",
        "\n",
        "def load_pickled_files(file_names):\n",
        "    for name in file_names:\n",
        "        yield pickle.load(open(name, \"rb\"))\n",
        "        \n",
        "\n",
        "def get_matching_files(name):\n",
        "    drive_files_response = _download_response_bytes(drive_service.files().list())\n",
        "    drive_files_response_dict = json.loads(\n",
        "        drive_files_response.decode('utf-8')\n",
        "    )\n",
        "    drive_files_dict = drive_files_response_dict['files']\n",
        "  \n",
        "    matching_records = [\n",
        "        record\n",
        "        for record in drive_files_dict\n",
        "        if record['name'] == name\n",
        "    ]\n",
        "\n",
        "    no_records = len(matching_records)\n",
        "    if no_records == 0:\n",
        "        raise ValueError('no such file: \"{}\" on your Google Drive'.format(name))\n",
        "    elif no_records > 1:\n",
        "        print('warning: multiple matches for file \"{}\"'.format(name))\n",
        "    return matching_records"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MaBpNAlhKjIO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "example_file_name = 'smsspamcollection.zip'\n",
        "example_file_id = get_file_id(example_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pZ4Th7DiKmgg",
        "colab_type": "code",
        "outputId": "55a9ead5-0419-4e4f-f586-0f67320950d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "move_from_drive_to_disk([example_file_name], ['smsspamcollection.zip'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading file: \"smsspamcollection.zip\"\n",
            "Downloaded 203415/203415 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O3uavcG3KqY2",
        "colab_type": "code",
        "outputId": "7b7cb2d4-b0b2-4782-e09a-38e99e1a1c09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip smsspamcollection.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Foe-7kNqKsWJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T7jU3_4RKvaA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load the dataset of sms messages\n",
        "df=pd.read_table('SMSSpamCollection',header = None , encoding='utf8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OR5rKANDLAKQ",
        "colab_type": "code",
        "outputId": "c88b3c37-9fbe-4a51-e638-9abf3f23b4c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "#print useful information about the dataset\n",
        "print(df.info())\n",
        "print(df.head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 2 columns):\n",
            "0    5572 non-null object\n",
            "1    5572 non-null object\n",
            "dtypes: object(2)\n",
            "memory usage: 87.1+ KB\n",
            "None\n",
            "      0                                                  1\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W0HP78-aLKP3",
        "colab_type": "code",
        "outputId": "5b674bff-002d-4e1d-f920-6bd05f49793c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#check class distribution\n",
        "classes = df[0]\n",
        "print(classes.value_counts())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham     4825\n",
            "spam     747\n",
            "Name: 0, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "im4erbnkOUGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Preprocess the Data**\n",
        "* ### Preprocessing the data is an essential step in natural language process. In the following cells, we will convert our class labels to binary values using the LabelEncoder from sklearn, replace email addresses, URLs, phone numbers, and other symbols by using regular expressions, remove stop words, and extract word stems. "
      ]
    },
    {
      "metadata": {
        "id": "IHOSXM70LVIj",
        "colab_type": "code",
        "outputId": "19ccb50a-1cf8-4a4f-b4b9-9ad8437f6de3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Preprocess the data\n",
        "#convert class lables to binary values , 0 = ham , 1=spam\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "Y = encoder.fit_transform(classes)\n",
        "print(classes[:10])\n",
        "print(Y[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0     ham\n",
            "1     ham\n",
            "2    spam\n",
            "3     ham\n",
            "4     ham\n",
            "5    spam\n",
            "6     ham\n",
            "7     ham\n",
            "8    spam\n",
            "9    spam\n",
            "Name: 0, dtype: object\n",
            "[0 0 1 0 0 1 0 0 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1YNIVIbkL4wD",
        "colab_type": "code",
        "outputId": "1fe27aaf-89cb-43d0-b4ad-48199088b0c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "#store the SMS message data\n",
        "text_messages = df[1]\n",
        "print(text_messages[:10])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    Go until jurong point, crazy.. Available only ...\n",
            "1                        Ok lar... Joking wif u oni...\n",
            "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3    U dun say so early hor... U c already then say...\n",
            "4    Nah I don't think he goes to usf, he lives aro...\n",
            "5    FreeMsg Hey there darling it's been 3 week's n...\n",
            "6    Even my brother is not like to speak with me. ...\n",
            "7    As per your request 'Melle Melle (Oru Minnamin...\n",
            "8    WINNER!! As a valued network customer you have...\n",
            "9    Had your mobile 11 months or more? U R entitle...\n",
            "Name: 1, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d_askiHEMLIk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# use regular expressions to replace email addresses, URLs, phone numbers, other numbers\n",
        "\n",
        "# Replace email addresses with 'email'\n",
        "processed = text_messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
        "                                 'emailaddress')\n",
        "\n",
        "# Replace URLs with 'webaddress'\n",
        "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
        "                                  'webaddress')\n",
        "\n",
        "# Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\n",
        "processed = processed.str.replace(r'£|\\$', 'moneysymb')\n",
        "    \n",
        "# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n",
        "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
        "                                  'phonenumbr')\n",
        "    \n",
        "# Replace numbers with 'numbr'\n",
        "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FYc0DlYKOw36",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove punctuation\n",
        "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
        "\n",
        "# Replace whitespace between terms with a single space\n",
        "processed = processed.str.replace(r'\\s+', ' ')\n",
        "\n",
        "# Remove leading and trailing whitespace\n",
        "processed = processed.str.replace(r'^\\s+|\\s+?$', '')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CP5FjuxHPGFs",
        "colab_type": "code",
        "outputId": "ca59147c-c50c-46eb-a2d4-b8058d020170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        }
      },
      "cell_type": "code",
      "source": [
        "# change words to lower case - Hello, HELLO, hello are all the same word\n",
        "processed = processed.str.lower()\n",
        "print(processed)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       go until jurong point crazy available only in ...\n",
            "1                                 ok lar joking wif u oni\n",
            "2       free entry in numbr a wkly comp to win fa cup ...\n",
            "3             u dun say so early hor u c already then say\n",
            "4       nah i don t think he goes to usf he lives arou...\n",
            "5       freemsg hey there darling it s been numbr week...\n",
            "6       even my brother is not like to speak with me t...\n",
            "7       as per your request melle melle oru minnaminun...\n",
            "8       winner as a valued network customer you have b...\n",
            "9       had your mobile numbr months or more u r entit...\n",
            "10      i m gonna be home soon and i don t want to tal...\n",
            "11      six chances to win cash from numbr to numbr nu...\n",
            "12      urgent you have won a numbr week free membersh...\n",
            "13      i ve been searching for the right words to tha...\n",
            "14                      i have a date on sunday with will\n",
            "15      xxxmobilemovieclub to use your credit click th...\n",
            "16                                 oh k i m watching here\n",
            "17      eh u remember how numbr spell his name yes i d...\n",
            "18      fine if that s the way u feel that s the way i...\n",
            "19      england v macedonia dont miss the goals team n...\n",
            "20               is that seriously how you spell his name\n",
            "21      i m going to try for numbr months ha ha only j...\n",
            "22         so ü pay first lar then when is da stock comin\n",
            "23      aft i finish my lunch then i go str down lor a...\n",
            "24      ffffffffff alright no way i can meet up with y...\n",
            "25      just forced myself to eat a slice i m really n...\n",
            "26                          lol your always so convincing\n",
            "27      did you catch the bus are you frying an egg di...\n",
            "28      i m back amp we re packing the car now i ll le...\n",
            "29      ahhh work i vaguely remember that what does it...\n",
            "                              ...                        \n",
            "5542             armand says get your ass over to epsilon\n",
            "5543                u still havent got urself a jacket ah\n",
            "5544    i m taking derek amp taylor to walmart if i m ...\n",
            "5545        hi its in durban are you still on this number\n",
            "5546             ic there are a lotta childporn cars then\n",
            "5547    had your contract mobile numbr mnths latest mo...\n",
            "5548                     no i was trying it all weekend v\n",
            "5549    you know wot people wear t shirts jumpers hat ...\n",
            "5550            cool what time you think you can get here\n",
            "5551    wen did you get so spiritual and deep that s g...\n",
            "5552    have a safe trip to nigeria wish you happiness...\n",
            "5553                           hahaha use your brain dear\n",
            "5554    well keep in mind i ve only got enough gas for...\n",
            "5555    yeh indians was nice tho it did kane me off a ...\n",
            "5556    yes i have so that s why u texted pshew missin...\n",
            "5557    no i meant the calculation is the same that lt...\n",
            "5558                                sorry i ll call later\n",
            "5559    if you aren t here in the next lt gt hours imm...\n",
            "5560                      anything lor juz both of us lor\n",
            "5561    get me out of this dump heap my mom decided to...\n",
            "5562    ok lor sony ericsson salesman i ask shuhui the...\n",
            "5563                               ard numbr like dat lor\n",
            "5564    why don t you wait til at least wednesday to s...\n",
            "5565                                            huh y lei\n",
            "5566    reminder from onumbr to get numbr pounds free ...\n",
            "5567    this is the numbrnd time we have tried numbr c...\n",
            "5568                  will ü b going to esplanade fr home\n",
            "5569    pity was in mood for that so any other suggest...\n",
            "5570    the guy did some bitching but i acted like i d...\n",
            "5571                            rofl its true to its name\n",
            "Name: 1, Length: 5572, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RBgk17BMPMKn",
        "colab_type": "code",
        "outputId": "70288394-929e-4249-ad2d-dc485047821d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> stopwords\n",
            "    Downloading package stopwords to /root/nltk_data...\n",
            "      Unzipping corpora/stopwords.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "dAQj9cYIPxla",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# remove stop words from text messages\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "processed = processed.apply(lambda x: ' '.join(\n",
        "    term for term in x.split() if term not in stop_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PxS9EEZyP2Z8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove word stems using a Porter stemmer\n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "processed = processed.apply(lambda x: ' '.join(\n",
        "    ps.stem(term) for term in x.split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZBinCnvWQrSu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **3. Generating Features**\n",
        "* ###  Feature engineering is the process of using domain knowledge of the data to create features for machine learning algorithms. In this project, the words in each text message will be our features. For this purpose, it will be necessary to tokenize each word. We will use the 1500 most common words as features."
      ]
    },
    {
      "metadata": {
        "id": "EUJhOa3NQcV4",
        "colab_type": "code",
        "outputId": "b34cba0b-0e43-42fb-a1ed-c0fd9d1411ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download() #punkt"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "DNCoBRplQehR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# create bag-of-words\n",
        "all_words = []\n",
        "\n",
        "for message in processed:\n",
        "    words = word_tokenize(message)\n",
        "    for w in words:\n",
        "        all_words.append(w)\n",
        "        \n",
        "all_words = nltk.FreqDist(all_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bkKfKLfGRPLD",
        "colab_type": "code",
        "outputId": "ae3fad6b-986c-417c-8dc1-b717a0398a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "#Print the total number words and the most 15 common words\n",
        "print('Number of words : {}'.format(len(all_words)))\n",
        "print('Most common words: {}'.format(all_words.most_common(15)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words : 6579\n",
            "Most common words: [('numbr', 2648), ('u', 1207), ('call', 674), ('go', 456), ('get', 451), ('ur', 391), ('gt', 318), ('lt', 316), ('come', 304), ('moneysymbnumbr', 303), ('ok', 293), ('free', 284), ('day', 276), ('know', 275), ('love', 266)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DIOJZUZRR0w-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# use the 1500 most common words as features\n",
        "word_features = list(all_words.keys())[:1500]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7DR18LIZSOIO",
        "colab_type": "code",
        "outputId": "fa85b5ee-3305-4fda-b526-71858386ab13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# The find_features function will determine which of the 1500 word features are contained in the review\n",
        "def find_features(message):\n",
        "    words = word_tokenize(message)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[word] = (word in words)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Lets see an example!\n",
        "features = find_features(processed[1])\n",
        "for key, value in features.items():\n",
        "    if value == True:\n",
        "        print(key)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ok\n",
            "lar\n",
            "joke\n",
            "wif\n",
            "u\n",
            "oni\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UAGtezrkSbIP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now lets do it for all the messages\n",
        "messages = list(zip(processed, Y))\n",
        "\n",
        "# define a seed for reproducibility\n",
        "seed = 1\n",
        "np.random.seed = seed\n",
        "np.random.shuffle(messages)\n",
        "\n",
        "# call find_features function for each SMS message\n",
        "featuresets = [(find_features(text), label) for (text, label) in messages]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ypTZIHHwS4dJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we can split the featuresets into training and testing datasets using sklearn\n",
        "from sklearn import model_selection\n",
        "\n",
        "# split the data into training and testing datasets\n",
        "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fJXp0tLQT9Ya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(len(training))\n",
        "print(len(testing))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8EuWDbJWnZPo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Scikit Learn Classifiers with NLTK**"
      ]
    },
    {
      "metadata": {
        "id": "yMs09-C3UfP-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8fbbc617-9628-4713-c69d-49fe5140ab34"
      },
      "cell_type": "code",
      "source": [
        "# We can use sklearn algorithms in NLTK\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
        "\n",
        "# train the model on the training data\n",
        "model.train(training)\n",
        "\n",
        "# and test on the testing dataset!\n",
        "accuracy = nltk.classify.accuracy(model, testing)*100\n",
        "print(\"SVC Accuracy: {}\".format(accuracy))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVC Accuracy: 98.7078248384781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cw4g_9p6nniI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d31df612-2bc6-4c2c-c8fc-3f2d56cc7e69"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Define models to train\n",
        "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
        "         \"Naive Bayes\", \"SVM Linear\"]\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    LogisticRegression(),\n",
        "    SGDClassifier(max_iter = 100),\n",
        "    MultinomialNB(),\n",
        "    SVC(kernel = 'linear')\n",
        "]\n",
        "\n",
        "models = zip(names, classifiers)\n",
        "\n",
        "for name, model in models:\n",
        "    nltk_model = SklearnClassifier(model)\n",
        "    nltk_model.train(training)\n",
        "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
        "    print(\"{} Accuracy: {}\".format(name, accuracy))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K Nearest Neighbors Accuracy: 95.04666188083274\n",
            "Decision Tree Accuracy: 97.48743718592965\n",
            "Random Forest Accuracy: 98.1335247666906\n",
            "Logistic Regression Accuracy: 98.77961234745155\n",
            "SGD Classifier Accuracy: 98.77961234745155\n",
            "Naive Bayes Accuracy: 98.27709978463747\n",
            "SVM Linear Accuracy: 98.7078248384781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qyIPypfzn2wp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2877e427-0433-47c4-b831-efa0596640cb"
      },
      "cell_type": "code",
      "source": [
        "# Ensemble methods - Voting classifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
        "         \"Naive Bayes\", \"SVM Linear\"]\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    LogisticRegression(),\n",
        "    SGDClassifier(max_iter = 100),\n",
        "    MultinomialNB(),\n",
        "    SVC(kernel = 'linear')\n",
        "]\n",
        "\n",
        "models = list(zip(names, classifiers))\n",
        "\n",
        "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
        "nltk_ensemble.train(training)\n",
        "accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
        "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Voting Classifier: Accuracy: 98.7078248384781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EUG9NfpZoLSR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e1df59c1-5656-46ba-b6c9-4058d4c1d3be"
      },
      "cell_type": "code",
      "source": [
        "# make class label prediction for testing set\n",
        "txt_features, labels = list(zip(*testing))\n",
        "\n",
        "prediction = nltk_ensemble.classify_many(txt_features)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nX1bD2Byo2og",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "b80d24df-b7c3-4519-f150-2744b8b5b7b3"
      },
      "cell_type": "code",
      "source": [
        "# print a confusion matrix and a classification report\n",
        "print(classification_report(labels, prediction))\n",
        "\n",
        "pd.DataFrame(\n",
        "    confusion_matrix(labels, prediction),\n",
        "    index = [['actual', 'actual'], ['ham', 'spam']],\n",
        "    columns = [['predicted', 'predicted'], ['ham', 'spam']])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.99      1.00      0.99      1209\n",
            "          1       0.99      0.92      0.95       184\n",
            "\n",
            "avg / total       0.99      0.99      0.99      1393\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>ham</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
              "      <th>ham</th>\n",
              "      <td>1207</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spam</th>\n",
              "      <td>15</td>\n",
              "      <td>169</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            predicted     \n",
              "                  ham spam\n",
              "actual ham       1207    2\n",
              "       spam        15  169"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "1pcerhA3pB56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}